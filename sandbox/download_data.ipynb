{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e96f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datalad.api as dl\n",
    "\n",
    "# hcp_path = \"/Users/Shared/hcp/human-connectome-project-openaccess/\"\n",
    "# ds = dl.Dataset(hcp_path)\n",
    "\n",
    "# results = ds.status(annex='all')\n",
    "\n",
    "# ds.get(\"HCP1200/116726\")\n",
    "# ds.get(\"HCP1200/118528\")\n",
    "# ds.get(\"HCP1200/118528/unprocessed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5581dd",
   "metadata": {},
   "source": [
    "# Make sure you have created HPC AWS credentials first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502f4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datalad.api as dl\n",
    "from datalad.api import Dataset\n",
    "from datetime import datetime\n",
    "from contextlib import contextmanager\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logs directory\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Configure base logging (this will catch setup_dataset logs)\n",
    "base_log_filename = f\"logs/hcp_setup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(base_log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@contextmanager\n",
    "def log_get_operation(subject, data_type, log_dir=\"logs\"):\n",
    "    \"\"\"\n",
    "    Context manager to create separate log file for each ds.get() operation\n",
    "    \n",
    "    Args:\n",
    "        subject: Subject ID (e.g., \"116726\")\n",
    "        data_type: Type of data being downloaded (e.g., \"T1w_MPR1\", \"tfMRI_MOTOR_LR\")\n",
    "        log_dir: Directory to save logs\n",
    "    \"\"\"\n",
    "    \n",
    "    data_type = data_type.replace('/', '_')\n",
    "\n",
    "\n",
    "    # Create unique log filename\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = f\"{log_dir}/get_{subject}_{data_type}_{timestamp}.log\"\n",
    "    \n",
    "    # Create file handler for this specific operation\n",
    "    file_handler = logging.FileHandler(log_filename)\n",
    "    file_handler.setLevel(logging.INFO)\n",
    "    file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    \n",
    "    # Get the root logger and add file handler\n",
    "    root_logger = logging.getLogger()\n",
    "    root_logger.addHandler(file_handler)\n",
    "    \n",
    "    # Log start of operation\n",
    "    logger.info(f\"Starting download: Subject {subject}, Data type {data_type}\")\n",
    "    logger.info(f\"Operation log file: {log_filename}\")\n",
    "    \n",
    "    try:\n",
    "        yield log_filename\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during download: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        logger.info(f\"Completed operation for Subject {subject}, Data type {data_type}\")\n",
    "        # Clean up - remove the file handler\n",
    "        root_logger.removeHandler(file_handler)\n",
    "        file_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9f5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "REPO_URL = \"https://github.com/datalad-datasets/human-connectome-project-openaccess\"\n",
    "# LOCAL_DATASET_PATH = \"/Users/zenkavi/hcp/\"\n",
    "LOCAL_DATASET_PATH = \"/Users/zenkavi/Documents/EnkaviLab/data/hcp/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94962b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataset(repo_url, local_path):\n",
    "    \"\"\"\n",
    "    Clone or update the HCP dataset\n",
    "    \"\"\"\n",
    "    logger.info(f\"Setting up dataset at {local_path}\")\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        logger.info(\"Dataset already exists, updating...\")\n",
    "        ds = Dataset(local_path)\n",
    "        try:\n",
    "            ds.update(merge=True)\n",
    "        except Exception as e:\n",
    "            if \"dubious ownership\" in str(e):\n",
    "                logger.error(\"Git ownership issue detected!\")\n",
    "                logger.error(\"To fix this, run:\")\n",
    "                logger.error(f\"git config --global --add safe.directory {os.path.abspath(local_path)}\")\n",
    "                logger.error(\"Or for all HCP subjects:\")\n",
    "                logger.error(f\"find {os.path.abspath(local_path)}/HCP1200 -maxdepth 1 -type d -exec git config --global --add safe.directory {{}} \\\\;\")\n",
    "                raise\n",
    "            else:\n",
    "                logger.warning(f\"Update failed: {e}. Continuing with existing dataset.\")\n",
    "    else:\n",
    "        logger.info(\"Cloning dataset...\")\n",
    "        try:\n",
    "            ds = dl.clone(repo_url, local_path)\n",
    "        except Exception as e:\n",
    "            if \"dubious ownership\" in str(e):\n",
    "                logger.error(\"Git ownership issue during clone!\")\n",
    "                logger.error(\"To fix this, run:\")\n",
    "                logger.error(f\"git config --global --add safe.directory {os.path.abspath(local_path)}\")\n",
    "                logger.error(\"Then re-run this script.\")\n",
    "                raise\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ddcffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def remove_unwanted_directories(ds, subject):\n",
    "    \"\"\"\n",
    "    Remove unwanted directories using file system operations\n",
    "    \"\"\"\n",
    "    \n",
    "    keep_patterns = [\n",
    "        \"T1w_MPR1\",\n",
    "        \"T1w_MPR2\", \n",
    "        \"T2w_SPC1\",\n",
    "        \"tfMRI_MOTOR_LR\",\n",
    "        \"tfMRI_MOTOR_RL\",\n",
    "        \"tfMRI_GAMBLING_LR\",\n",
    "        \"tfMRI_GAMBLING_RL\"\n",
    "    ]\n",
    "    \n",
    "    rm_paths = [\"3T\", \"7T\", \"MEG\"]\n",
    "\n",
    "    for rm_path in rm_paths:\n",
    "        logger.info(f\"Removing unwanted data for subject {subject}\")\n",
    "        \n",
    "        subject_rm_path = Path(ds.path) / \"HCP1200\" / subject / \"unprocessed\" / rm_path\n",
    "        \n",
    "        if subject_rm_path.exists():\n",
    "            all_dirs = [d for d in subject_rm_path.iterdir() if d.is_dir()]\n",
    "            \n",
    "            for dir_path in all_dirs:\n",
    "                dir_name = dir_path.name\n",
    "                \n",
    "                if dir_name not in keep_patterns:\n",
    "                    logger.info(f\"Removing directory: {dir_path}\")\n",
    "                    try:\n",
    "                        shutil.rmtree(dir_path)\n",
    "                        logger.info(f\"Successfully removed {dir_name} for {subject}\")\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"Failed to remove {dir_name} for {subject}: {e}\")\n",
    "                else:\n",
    "                    logger.info(f\"Keeping directory: {dir_name} for {subject}\")\n",
    "        else:\n",
    "            logger.warning(f\"Subject rm directory not found: {subject_rm_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82023f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your data specifications\n",
    "subject_list = [\"116726\", \"118528\"]\n",
    "# subject_list = [\"116726\", \"118528\", \"131823\", \"135124\", \"136126\", \"150524\", \"152225\", \"167743\", \"176542\"]\n",
    "\n",
    "# Setup dataset once\n",
    "# LOCAL_DATA_PATH shouldn't exist before this\n",
    "ds = setup_dataset(REPO_URL, LOCAL_DATASET_PATH)\n",
    "\n",
    "# Download each subject's data with separate logs\n",
    "for subject in subject_list:\n",
    "    logger.info(f\"Processing subject {subject}\")\n",
    "\n",
    "    patterns = [f\"HCP1200/{subject}/unprocessed/3T/T1w_MPR1/\", \n",
    "                f\"HCP1200/{subject}/unprocessed/3T/T1w_MPR2/\",\n",
    "                f\"HCP1200/{subject}/unprocessed/3T/T2w_SPC1/\",\n",
    "                f\"HCP1200/{subject}/unprocessed/3T/tfMRI_MOTOR_LR/\",\n",
    "                f\"HCP1200/{subject}/unprocessed/3T/tfMRI_MOTOR_RL/\",\n",
    "                f\"HCP1200/{subject}/unprocessed/3T/tfMRI_GAMBLING_LR/\",\n",
    "                f\"HCP1200/{subject}/unprocessed/3T/tfMRI_GAMBLING_RL/\"]\n",
    "    \n",
    "    # Download structural data\n",
    "    for pattern in patterns:        \n",
    "        with log_get_operation(subject, pattern) as log_file:\n",
    "            try:\n",
    "                result = ds.get(pattern)\n",
    "                logger.info(f\"Successfully downloaded {pattern}\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download {pattern}\")\n",
    "        remove_unwanted_directories(ds, subject)\n",
    "    \n",
    "\n",
    "logger.info(\"All downloads completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68810378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLEST CASE\n",
    "\n",
    "# import datalad.api as dl\n",
    "\n",
    "# REPO_URL = \"https://github.com/datalad-datasets/human-connectome-project-openaccess\"\n",
    "# LOCAL_DATASET_PATH = \"/Users/zenkavi/hcp/\"\n",
    "\n",
    "# ds = dl.clone(REPO_URL, LOCAL_DATASET_PATH)\n",
    "\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/T1w_MPR1/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/T1w_MPR2/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/T2w_SPC1/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/tfMRI_MOTOR_LR/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/tfMRI_MOTOR_RL/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/tfMRI_GAMBLING_LR/\")\n",
    "# ds.get(\"HCP1200/116726/unprocessed/3T/tfMRI_GAMBLING_RL/\")\n",
    "\n",
    "# remove_unwanted_directories(ds, \"116726\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fa920",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94db71e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dartbrains",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
